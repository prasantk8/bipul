# ğŸš€ Chapterâ€¯8 â€” **Datasetâ€¯Engineering & Dataâ€‘Centric AI**

> **Goal** â€“ give you a *field manual* for curating, generating, augmenting, and governing data that makesâ€”or breaksâ€”foundationâ€‘model performance.  We weave together lessons from GPTâ€‘4, Gemini, Llamaâ€‘3, enterprise deployments (banking, healthcare), and 2024â€‘2025 research.

---

## 1ï¸âƒ£  Fast Glossary

| Term                             | Definition                                                                               | Production Impact                       |
| -------------------------------- | ---------------------------------------------------------------------------------------- | --------------------------------------- |
| **Dataâ€‘Centric AI**              | Optimise model quality by improving data, not model code.                                | 10Ã— cheaper than scaling params.        |
| **Dataset Curation**             | Process of collecting, filtering, deduplicating, classâ€‘balancing, & annotating raw data. | Determines ceiling of model capability. |
| **Selfâ€‘Supervised Data**         | Unlabeled sequences used for pre/continued training.                                     | Abundant but noisy.                     |
| **Instruction Data (SFT)**       | `(instruction, input, output)` triples.                                                  | Teaches model task formats & style.     |
| **Preference / Comparison Data** | `(prompt, chosen, rejected)` tuples.                                                     | Train reward models (RLHF / DPO).       |
| **Synthetic Data**               | Modelâ€‘generated examples, optionally humanâ€‘vetted.                                       | Bootstraps scarce domains cheaply.      |
| **Data Lineage / Provenance**    | Traceability of each recordâ€™s source & license.                                          | Compliance & reproducibility.           |
| **Overlap / Leakage**            | Portion of eval set present in training data.                                            | Inflates benchmarks; must audit.        |
| **Toxicity / Bias Filter**       | Classifier or rule set removing harmful text.                                            | Safety & legal risk mitigation.         |
| **Deduplication**                | Remove nearâ€‘identical shards (SimHash, MinHash).                                         | Prevents overâ€‘memorisation.             |

---

## 2ï¸âƒ£  The Dataâ€‘Centric Lifecycle

```mermaid
flowchart TB
  A[Raw Sources] --> B(Curation & Cleaning)
  B --> C[Split Train / Val / Test]
  C --> D(Preâ€‘training)
  C --> E(SFT / RLHF)
  D & E --> F(Eval & Audit)
  F -->|Drift / Gaps| B
```

> **Insight** â€“ treat data pipelines like code: version, test, diff, roll back.

---

## 3ï¸âƒ£  Source Acquisition

| Source                        | Strengths                         | Risks                                | Notes                                          |
| ----------------------------- | --------------------------------- | ------------------------------------ | ---------------------------------------------- |
| **Commonâ€¯CrawlÂ CCâ€‘Net**       | Huge, multilingual.               | Noisy, disinfo.                      | Filter by `quality > 0.8`, topâ€‘level domains.  |
| **Books3**                    | Wellâ€‘edited prose style.          | Copyright litigation (2024 verdict). | Use openâ€‘licensed subsets (ProjectÂ Gutenberg). |
| **StackOverflowÂ 2024Â Dump**   | Highâ€‘quality code Q\&A.           | License: CCâ€‘BYâ€‘SA.                   | Retain attribution per clause.                 |
| **PubMed Central**            | Biomedical abstracts & fullâ€‘text. | Domain heavy jargon.                 | Great for medical LLMs.                        |
| **Internal Corp Docs**        | Proprietary knowledge.            | PII, trade secrets.                  | Redact, tenancy isolation.                     |
| **Synthetic (Selfâ€‘Instruct)** | Infinite supply.                  | Quality variance, domain drift.      | Human curate 5â€‘10â€¯% seed.                      |

---

## 4ï¸âƒ£  Data Curation Pipeline (Template)

1. **Ingest â†’** fetch & store raw docs (S3, Deltaâ€‘Lake).
2. **Text Extraction â†’** PDFâ†’text, HTML boilerplate removal.
3. **Language ID & Sharding â†’** fastText langâ€‘id; route to perâ€‘lang folders.
4. **Deduplication â†’** MinHashÂ LSH (thresholdâ€¯=â€¯0.8).
5. **Toxicity & PII Filter â†’**

   * OpenAI Moderation API or PerspectiveÂ v4.0
   * Presidio anonymiser (names, emails, govâ€¯IDs).
6. **Metadata Enrichment â†’** source, license, timestamp, tags.
7. **Quality Scoring â†’** perplexity (small LM), readability (Flesch), doc length.
8. **Stratified Split â†’** maintain domain/lang class balance in train/val/test.
9. **Version & Snapshot â†’** DVC or LakeFS commit; tag with semantic version.

---

## 5ï¸âƒ£  Quantity vs Quality Tradeâ€‘off

| Scenario             | Empirical Finding (2024â€‘25 papers)                                       |
| -------------------- | ------------------------------------------------------------------------ |
| Webâ€‘scale preâ€‘train  | Qualityâ€‘filtered 15â€¯B tokens matched 100â€¯B raw tokens (GunasekarÂ 2023).  |
| Domain models (<7â€¯B) | Better to oversample highâ€‘quality narrow corpus than dilute with CCâ€‘Net. |
| Code LLM             | 7â€¯B tokens curated code > 1â€¯T mixed tokens.                              |

**Heuristic** â€“ token budget âˆ (model params) Ã— (4â€‘5).  Beyond that, diminishing returns unless *new* domain.

---

## 6ï¸âƒ£  Synthetic Data Strategies

### 6.1  Selfâ€‘Instruct 2.0 Workflow

1. Seed 1â€¯K human tasks.
2. UseÂ GPTâ€‘4o to autoâ€‘generate 10Ã— new tasks.
3. Deduplicate & qualityâ€‘rank tasks; keep top 30â€¯%.
4. Human spotâ€‘check 5â€¯% for safety.

### 6.2  Roleâ€‘Play Conversations

* Use multiâ€‘model chat (e.g., two Llamaâ€‘3â€‘Instruct) to produce realistic dialogues.
* Filter with perplexity and toxicity classifier.

### 6.3  Synthetic Code Explanations

* Prompt LLM to add inline comments for raw GitHub functions.
* Evaluated with BLEU against human comment subset.

> ğŸ” **Metric** â€“ *Diversity Score* (Unique nâ€‘gram ratio) ensures synthetic set isnâ€™t repetitive.

---

## 7ï¸âƒ£  Compliance, Ethics & Governance

| Area                     | Best Practice                                                  |
| ------------------------ | -------------------------------------------------------------- |
| **License Tracking**     | Maintain SPDX doc; record SPDX ID per file.                    |
| **Sensitive Domains**    | Medical, legal â†’ add SME review gate.                          |
| **Geoâ€‘Restricted Data**  | Respect dataâ€‘residency (EUâ€‘GDPR, UAEÂ 2025 AIÂ Law).             |
| **Attribution**          | Inject citation metadata; enable downstream user credit.       |
| **Transparency Reports** | Publish highâ€‘level stats (language %, domain %, filter rates). |

---

## 8ï¸âƒ£  Evaluation of Data Quality

| Metric                | Tool                                    | Target                      |
| --------------------- | --------------------------------------- | --------------------------- |
| **Coverage**          | Stratified histogram over domains.      | matches production queries. |
| **Noise Rate**        | Manual label 1â€¯K sample; compute bad %. | <â€¯2â€¯%.                      |
| **Toxic Rate**        | Perspective severe\_toxicity.           | <â€¯0.5â€¯%.                    |
| **Overlap with Eval** | NearDup ratio via cosine sim â‰¥â€¯0.9.     | 0â€¯%.                        |
| **Information Gain**  | Î” perplexity on val set after 1 epoch.  | >â€¯0.5 nat.                  |

Automate with **DataComp Evaluator** scripts + Great Expectations rules.

---

## 9ï¸âƒ£  Advanced 2025 Techniques

1. **Active Data Selection** â€“ compute gradientâ€‘based influence scores; keep most informative samples (ZhangÂ 2024).
2. **Mixtureâ€‘ofâ€‘Dataset Weighting** â€“ train Pâ€‘router predicting which dataset best improves next batch (Googleâ€¯Geminiâ€‘Fusion).
3. **ContrastiveÂ DataÂ Refinement (CDR)** â€“ pair positive/negative passages to teach retrieval models.
4. **LLMâ€‘powered Filter Chains** â€“ small model ranks, large model explains removal verdict; human confirm only controversial cases.
5. **Data Wallets** â€“ blockchainâ€‘based provenance + microâ€‘payments to original creators (UbiquityÂ 2025 pilot).

---

## ğŸ”Ÿ  Stepâ€‘byâ€‘Step Implementation Checklist

1. **Define Useâ€‘Case & Coverage Goals**.
2. **Inventory Potential Sources** â€“ open, licensed, internal.
3. **Bootstrap Curation Pipeline** (ETL code repo).
4. **Run Quality Audit v0** â€“ noise, toxicity, PII.
5. **Create SplitÂ v1**, tag version.
6. **Train Probe Model** â€“ small 1â€‘B to gauge perplexity gains.
7. **Iterate Filters / Data Mix** until val perplexity plateau.
8. **Launch Finetune/RAG** with datasetÂ v\*.
9. **Monitor Production Drift** â€“ track retrieval recall, OOD queries.
10. **Refresh & Reâ€‘version** quarterly or on schema change.

---

## ğŸ”š  Key Takeaways

* **Data quality is the strongest signal multiplier** â€“ garbage in, hallucinations out.
* Automate but **always keep human eyes** on samples for critical domains.
* Track lineage & license from dayÂ 1 to avoid costly rewrites.
* Use synthetic data judiciously; measure diversity & utility.
* Data pipelines deserve CI/CD, observability, and security equal to model code.

> â€œThe model *is* the data.â€ â€”Â AndrewÂ Ng, 2021
