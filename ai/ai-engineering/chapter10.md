# ğŸš€ Chapter 10 â€” **AI Engineering Architecture & User Feedback Loops**

> **Aim**Â â€” deliver an actionable blueprint for designing **scalable, monitored, and continuously improving** AI applications.  This chapter stitches every prior concept (prompting, RAG, finetune, inference) into an endâ€‘toâ€‘end architecture, plus feedback mechanisms that turn usage data into model gains.

---

## 1ï¸âƒ£  Architecture Progression (From Simple â†’ Enterpriseâ€‘Grade)

### 1.1  V0: Direct Call Pattern

```mermaid
flowchart LR
  U(User) -->|prompt| M(Model API)
  M -->|response| U
```

* **When OK**: internal PoC, hackathons.
* **Limitations**: no context, no guardrails, no observability.

### 1.2  V1: RAG + Guardrails

```mermaid
flowchart LR
  U --> R(Retriever)
  R --> P(Prompt Assembler)
  P --> G(InputÂ Guard)
  G --> M
  M --> O(OutputÂ Guard)
  O --> U
```

* Adds retrieval and safety layers; start logging metrics.

### 1.3  V2: Model Router & Gateway (Multiâ€‘Model)

```mermaid
flowchart LR
  subgraph Gateway
    U --> AU(Auth & Rateâ€‘limit)
    AU --> RT(Router)
  end
  RT -->|cheap| Small(SmallÂ LLM)
  RT -->|complex| Big(BigÂ LLM)
  RT -->|code| Code(CoderÂ LLM)
```

* Choose model per task: saves cost up to 70â€¯% (ShopifyÂ BrokerÂ 2024).

### 1.4  V3: Orchestrated Agent Graph

* **Frameworks**: LangGraph, CrewAI, AutogenÂ 2.
* Nodes = tools / LLM calls, edges = conditions.
* Example: Tripâ€‘planner agentÂ â†’ PlannerÂ LLM â†’ FlightÂ API â†’ HotelÂ RAG â†’ PriceÂ Tool.

---

## 2ï¸âƒ£  Core Architectural Components & Design Tips

| Layer               | Key Choices                                            | Pitfalls                                  |
| ------------------- | ------------------------------------------------------ | ----------------------------------------- |
| **Gateway**         | Auth, rateâ€‘limit, request schema validation.           | JSONÂ schema drift.                        |
| **Context Builder** | RAG, tool results, conversation memory.                | Oversized context â†’ cost/latency blowâ€‘up. |
| **LLM Router**      | Ruleâ€‘based, embedding similarity, or small classifier. | Misâ€‘routing hits accuracy & cost.         |
| **Guardrails**      | Moderation, PII mask, format validator.                | False positives block legit queries.      |
| **Observability**   | Trace each hop; token usage; userÂ feedback.            | No sampling â†’ ğŸ’¾ explosion.               |
| **Feedback Store**  | Vote logs, error spans, embeddings of bad cases.       | GDPR delete requests.                     |

Realâ€‘WorldÂ ExampleÂ â€”Â **GitHub Copilot** pipeline attaches telemetry: IDE events â†’ anonymised tokens â†’ evalÂ cluster â†’ nightly alignment data.

---

## 3ï¸âƒ£  Feedback Loop Taxonomy

| Feedback Type           | Collection Method                  | Useâ€‘Case                          |
| ----------------------- | ---------------------------------- | --------------------------------- |
| **ThumbsÂ Up/Down**      | UI button                          | RLHF fineâ€‘tune refresh.           |
| **Corrected Output**    | Inline edit capture (Notion AI)    | Supervised SFT dataset v2.        |
| **Implicit Metrics**    | Timeâ€‘toâ€‘resolve, followâ€‘up queries | Product KPI, A/B prompt tuning.   |
| **Tool Outcome**        | Did tool call succeed (HTTPÂ 200)?  | Reward function shaping.          |
| **Conversation Reâ€‘use** | Retrieval of past chats            | Longâ€‘term memory personalization. |

### 3.1  Storage Design

* Appendâ€‘only `feedback.parquet` with fields: `user_id` (hash), `prompt_hash`, `model_id`, `latency_ms`, `score`, `corrected_text`, timestamp.
* Apply **deltaâ€‘merge** every 24â€¯h to training lake (LakeFS) â†’ triggersÂ CI pipeline.

### 3.2  Learning Pipeline

```text
Daily Cron â†’ Pull new feedback â†’ Filter highâ€‘quality â€¢ Dedup â†’ Mix with offline eval failures â†’ Train LoRA adapters â†’ Canary deploy via feature flag â†’ Online A/B metrics â†’ Promote if wins.
```

*Spotify DJ* uses this loop; 2â€‘day iteration, 0.6â€¯pp weekly CSAT gain.

---

## 4ï¸âƒ£  Realâ€‘World Case Studies

### 4.1  KhanÂ Academy Khanmigo

* **Architecture**: RAG + Mathâ€‘Tool + RLHF loops.
* **Feedback**: Student thumbs/down & teacher flags.
* **Result**: MathÂ score improvement 15â€¯%.  Retraining weekly on 50â€¯K new interactions.

### 4.2  Zendesk Answer Bot (Enterprise 2025)

* Router directs billing Qâ€™s to small FAQ LLM, tech Qâ€™s to big model.
* Live user edits captured; diff tool autoâ€‘labels good answers.
* Cost/answer â†“Â 62â€¯%; deflection rate â†‘Â 9â€¯pp.

### 4.3  Stripe Docsâ€‘AI

* Combines code execution tool to test CURL examples.
* Feedback = API error vs success logs; model autoâ€‘fixes docs.

---

## 5ï¸âƒ£  Monitoring & SLOs

| Signal                | Healthy Range | Dash Tool            |
| --------------------- | ------------- | -------------------- |
| p95 Latency           | <Â 800â€¯ms      | Prometheus + Grafana |
| TokenÂ / Query         | trending flat | BigQuery Looker      |
| Moderation Violations | <Â 0.02â€¯%      | OpenAI mod exporter  |
| Groundedness          | >Â 0.8 (RAGAS) | LangSmith dashboards |
| User CSAT             | +1â€¯pp MoM     | Amplitude events     |

Alert rules (YAML):

```yaml
- alert: high_moderation_hits
  expr: sum(rate(mod_violation_total[5m])) > 5
  for: 10m
  labels: { severity: critical }
```

---

## 6ï¸âƒ£  Security & Multiâ€‘Tenancy

1. **Tenantâ€‘aware RAG filters** (rowâ€‘level ACL).
2. **Prompt fingerprinting** to block repeated jailbreaks (hash user input).
3. **Canary Killswitch**Â â€” feature flag to drain traffic from buggy model.
4. **Differential Privacy Noise** on logs for regulatory markets (EU AI Act).

---

## 7ï¸âƒ£  Cost Governance

* **Budget Alerts**: tokens/day > budgetÂ â†’ route to cheaper model.
* **Shadow Mode**: test new model behind scenes, only 5â€¯% cost overhead.
* **Orgâ€‘level Quotas** (OpenAI)Â â†’ prevent bill-shock.

Large bank POC (2025) saved \$1.2â€¯M/yr by tiering: 70â€¯% queries answered by 7â€¯B openâ€‘source model, rest escalated.

---

## 8ï¸âƒ£  Advanced 2025 Architectures

| Innovation                 | Description                                                      | Example                  |
| -------------------------- | ---------------------------------------------------------------- | ------------------------ |
| **Model Mesh**             | Perâ€‘request weights streamed justâ€‘inâ€‘time to GPU pods.           | Google Pathways Serving. |
| **Edge Hybrid**            | Firstâ€‘pass summary on device, escalate to cloud for deep answer. | Apple iOSÂ 19 Siri.       |
| **Eventâ€‘Driven Functions** | LLM chooses AWSÂ Lambda to run code; payâ€‘perâ€‘millisecond.         | IKEA AR catalog bot.     |
| **Live RL (Bandit)**       | Router learns cheapest model that meets success prob per user.   | TikTok Liveâ€‘AI captions. |

---

## 9ï¸âƒ£  Implementation Checklist

1. **Map User Journeys** & required model skills.
2. **Design Router Rules** (regex, classifier, cost tier).
3. **Add Guardrails + Logging** early.
4. **Establish Feedback Schema & Store**.
5. **Deploy v0** (small traffic) & set baseline metrics.
6. **Wire Continuous Trainâ€‘Eval Loop** (weekly).
7. **Iterate** architecture layers only as complexity justifies.

---

## ğŸ”š  Takeaways

* Start simple; add RAG, router, agent layers incrementally.
* Feedback is *data gold*Â â€”Â mine it for continual learning.
* Observability, security, and cost controls must ship with v1, not retrofitted.
* Routing + model mix often yields 50â€‘70â€¯% inference \$ savings.

> "Architect for today, instrument for tomorrow, learn forever."
