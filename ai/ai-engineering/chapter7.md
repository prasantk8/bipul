# ğŸš€ Chapterâ€¯7 â€” **Finetuning & Parameterâ€‘Efficient Adaptation**

> **Objective** â€“ equip you with a *blueprint* to adapt foundation models to your domain **safely, cheaply, and scalably**.  We cover classic fullâ€‘model finetuning, modern PEFT techniques (LoRA, QLoRA, Adapters, Promptâ€‘Tuning), practical hardware tips, and decision frameworks (RAGâ€¯vsâ€¯Finetune).

---

## 1ï¸âƒ£  Quick Glossary

| Term                                      | Working Definition                                                               | ProdÂ Relevance                                |
| ----------------------------------------- | -------------------------------------------------------------------------------- | --------------------------------------------- |
| **Finetuning**                            | Further training a preâ€‘trained model on taskâ€‘specific data (updates weights).    | Unlock domain style, format adherence.        |
| **SFT (SupervisedÂ Finetuning)**           | Train on `(instruction, response)` pairs.                                        | First stage of most instruction models.       |
| **RLHF**                                  | Reinforcement Learning from Human Feedback; optimise responses ranked by humans. | Aligns model with human preference & safety.  |
| **RewardÂ Model (RM)**                     | Model predicting human preference score for a response.                          | Used as reward fn in RLHF or DPO.             |
| **PEFT (Parameterâ€‘Efficient Finetuning)** | Adapt model by training a small subset/adapter of parameters.                    | 10â€‘100Ã— cheaper vs full fineâ€‘tune.            |
| **LoRA**                                  | Inject lowâ€‘rank matrices into attention layers; backâ€‘prop only these.            | GPU mem â†“, training time â†“.                   |
| **QLoRA**                                 | Combine 4â€‘bit quantisation + LoRA; fit 65B on single GPU.                        | Stateâ€‘ofâ€‘practice for < \$100 budgets.        |
| **Adapters / IA3**                        | Lightweight MLP/attention scalars trained perâ€‘task.                              | Enables multiâ€‘task â€œadapter zooâ€.             |
| **Prefix / PromptÂ Tuning**                | Learn virtual tokens prepended to every input; base weights frozen.              | Fast hyperâ€‘personalisation (marketing copy).  |
| **BitFit**                                | Update **only** bias terms of layers.                                            | Memoryâ€‘minimal hack; good for some NLU tasks. |
| **CatastrophicÂ Forgetting**               | Model loses original capabilities during finetune.                               | Mitigate by low LR, adapter isolation.        |

---

## 2ï¸âƒ£  Finetune vs RAG Decision Matrix

| Scenario                                      | Recommend           | Rationale                                        |
| --------------------------------------------- | ------------------- | ------------------------------------------------ |
| Need **private knowledge** that changes daily | **RAG**             | Cheaper to refresh index than retrain.           |
| Need **strict output format / code style**    | **Finetune (LoRA)** | Model must learn pattern, not just recall facts. |
| Need **lowâ€‘latency, onâ€‘device**               | **PEFT + quantise** | Avoid external retrieval; smaller model.         |
| Heavy reasoning on **static domain** corpus   | **SFT + RLHF**      | Embed chainâ€‘ofâ€‘thought & style.                  |

**Hybrid** is common: finetune for style, *plus* RAG for freshness.

---

## 3ï¸âƒ£  Finetuning Pipelines

### 3.1  Classic 3â€‘Stage (OpenAI â€œInstructâ€ recipe)

```
BaseÂ Model  âœ 1.Â SFT âœ 2.Â RMÂ training âœ 3.Â RLHF (PPO) âœ AlignedÂ Model
```

* *SFT*: 5â€‘100â€¯K highâ€‘quality pairs.
* *RM*: train on `(prompt, â€‘preferred, +preferred)` triples.
* *RLHF*: optimise with PPO or DPO (DirectÂ PreferenceÂ Optimisation 2023).

### 3.2  PEFT Workflow (LoRA / QLoRA)

```
DataÂ Prep â†’ Quantised Checkpoint â†’ InjectÂ LoRAÂ adapters â†’ Train adapters (FP16 optimizer) â†’ Merge & Save (optional)
```

* Use **bitsandbytes 0.43+** for 4â€‘bit NF4 quant.
* **LoRA Config**: r=16, Î±=32, dropout=0.05 ; tune on evalâ€‘set.

> ğŸ”§ **Hardware tip** â€“ A100Â 40â€¯GB fits QLoRA of 70â€¯B (group sizeÂ 128) batchÂ =Â 4.

---

## 4ï¸âƒ£  Memory & Compute Optimisations

| Technique              | GPUÂ Mem â†“                | Comment                    |
| ---------------------- | ------------------------ | -------------------------- |
| 4â€‘bitÂ (NF4) quant      | â‰ˆÂ 4Ã—                     | Keep FP16 for LayerNorm.   |
| LoRA rankÂ 8            | 1â€‘2â€¯% params             | Interpolates well.         |
| DeepSpeed ZeROâ€‘3       | 4â€‘8Ã— (shardÂ opt & grads) | Multinode, slower startup. |
| FlashAttentionâ€‘2       | 1.2â€‘1.5Ã— speed           | Compile with SMÂ 8.0+.      |
| Gradient Checkpointing | 50â€¯% mem                 | Adds compute overhead.     |

---

## 5ï¸âƒ£  Data Engineering for Finetune

### 5.1  DataÂ Schema

| Stage | Expected Format                                       |
| ----- | ----------------------------------------------------- |
| SFT   | `{ "instruction": str, "input": str, "output": str }` |
| RM    | `{ "prompt": str, "chosen": str, "rejected": str }`   |
| DPO   | same as RM                                            |

### 5.2  Quality Signals

* Deduplicate nearâ€‘duplicate pairs (MinHash).
* Filter profanity / PII (Presidio, OpenAI mod).
* Prefer *longer* answers for reasoning tasks (CoT).
* Balance class labels for classification to avoid bias.

### 5.3  Synthesis Strategies when Lacking Data

* **Selfâ€‘Instruct** (WangÂ 2023) â€“ LLM generates tasks & answers, humans curate 5â€‘10â€¯%.
* **DataÂ Augmentation** â€“ paraphrase inputs via backâ€‘translation.
* **Toolâ€‘Assisted Labeling** â€“ use regex/search to autoâ€‘label obvious negatives.

---

## 6ï¸âƒ£  Evaluation & Overfitting Checks

| Metric Bucket           | Example Metric           | PassÂ Criteria |
| ----------------------- | ------------------------ | ------------- |
| **Task**                | EM / F1 on heldâ€‘out set  |               |
| **Style/Format**        | JSON schema parse rate   | â‰¥Â 0.98        |
| **Safety**              | Toxicity (Perspective)   | â‰¤Â 0.01        |
| **Catastrophic Forget** | LAMBADA perplexity delta | Î”Â <Â 5â€¯%       |
| **Compute**             | ThroughputÂ tokens/s      |               |

* Use **gptâ€‘eval** + **human spotâ€‘checks**.
* Track eval every epoch; earlyâ€‘stop when val loss plateaus.

---

## 7ï¸âƒ£  Deployment Patterns

### 7.1  Mergeâ€‘andâ€‘Serve (LoRA â†’ Base)

Pros: single checkpoint.
Cons: lose modularity; need FP16 for merge.

### 7.2  AdapterÂ Stack

Load base weights frozen; hotâ€‘swap adapters per tenant.
Pros: multiâ€‘tenant SaaS w/ MBâ€‘level adapters.

### 7.3  Onâ€‘Device Personal LoRA

Mobile/edge; 8â€‘bit base + 4â€‘bit LoRA.
Used by Canva MagicÂ Design offline.

---

## 8ï¸âƒ£  Security & Governance

| Risk                            | Mitigation                                          |
| ------------------------------- | --------------------------------------------------- |
| **Data licensing violation**    | Audit dataset provenance; maintain SPDX manifest.   |
| **Weights exfiltration**        | Encrypt checkpoints at rest; KMS keys.              |
| **Finetune causing toxic bias** | Include adversarial eval set; apply reward shaping. |
| **Catastrophic forgetting**     | Low LR, L2â€‘reg, freeze LayerNorm/embeds.            |

---

## 9ï¸âƒ£  Cost Planning Cheat Sheet

| Model Size | Full FP16 Cost\* | QLoRA Cost\* | AWS p4d/hr |
| ---------- | ---------------- | ------------ | ---------- |
| 7â€¯B        | â‰ˆÂ \$20           | \$4          | 1Â GPU 5â€¯h  |
| 13â€¯B       | \$60             | \$8          | 1Â GPU 12â€¯h |
| 70â€¯B       | \$650            | \~\$45       | 8Â GPU 24â€¯h |

\*rough at \$3/hr per A100Â 40G

---

## ğŸ”Ÿ  Advanced Research Trends (2025)

1. **MoEâ€‘Adapter Fusion** â€“ selective routing of tokens to finetuned experts.
2. **DifferentiableÂ Memory Patches** â€“ attach KV memory trained for recall tasks.
3. **SyntheticÂ SFT via Agents** â€“ selfâ€‘play task generation >200â€¯K examples.
4. **Sparse Finetune (LoRAâ€‘SV)** â€“ zero out <1â€¯% leastâ€‘used adapter rows at runtime.
5. **Crossâ€‘Lingual Adapter Reâ€‘use** â€“ train on English, transfer to 30 langs via small delta.

---

## ğŸªœ  Stepâ€‘byâ€‘Step Implementation Checklist

1. **Define Success Metrics** (ex: JSON validityÂ â‰¥Â 99â€¯%).
2. **Collect & Clean 2â€‘20â€¯K exemplars.**
3. **Choose PEFT method** (LoRA rank 16, 4â€‘bit).
4. **Spinâ€‘up training env** (ftâ€‘script + weightsÂ &Â biases).
5. **Run 1â€‘2 epoch pilot** â€“ validate loss curve.
6. **Evaluate on heldâ€‘out + safety set.**
7. **Hyperâ€‘tune r, Î±, LR.**
8. **Merge/Deploy adapter; A/B test vs base.**
9. **Monitor drift; schedule monthly recoil.**

---

## ğŸ”š  Key Takeaways

* **PEFT delivers 90â€¯% of gains for 10â€¯% of cost** â€“ default choice.
* **Data quality trumps model size**.
* **Evaluate breadth**â€“ style, safety, forgetting.
* **Merge adapters only when necessary** â€“ keep flexibility.

> â€œTrain small, ship fast, iterate always.â€
