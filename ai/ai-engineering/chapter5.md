# ğŸš€ Chapter 5: Prompt Engineering

Prompt engineering is the craft of **designing, structuring, and safeguarding** the instructions that drive foundationâ€‘model behaviour. Because modern models already possess vast latent capabilities, the right prompt often unlocks surprising performance without altering model weights.

---

## ğŸ“Œ Core Terminology & Definitions

| Term                                    | Explanation                                                                                                              |
| --------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ |
| **Prompt**                              | The full instruction sent to a model (may include system, user, toolâ€‘calling sections).                                  |
| **Task Description**                    | Naturalâ€‘language specification of what the model should do, including role and output format.                            |
| **Shot**                                | One *(input â†’ output)* example embedded inside the prompt. Zeroâ€‘shot â‡’â€¯no examples; fewâ€‘shot â‡’â€¯â‰¤â€¯k examples (kâ‰ªdataset). |
| **Inâ€‘Context Learning (ICL)**           | The model infers patterns from examples inside the *context window* instead of weight updates.                           |
| **Chainâ€‘ofâ€‘Thought (CoT)**              | Technique that asks a model to think stepâ€‘byâ€‘step before emitting a final answer.                                        |
| **Selfâ€‘Consistency**                    | Sampling multiple CoT traces, then majorityâ€‘voting on final answers for higher reliability.                              |
| **Temperature / Topâ€‘k / Topâ€‘p**         | Sampling knobs controlling randomness vs. determinism in generation.                                                     |
| **System Prompt**                       | Highâ€‘level, hidden instruction that sets global behaviour (e.g., brand voice, safety policy).                            |
| **Prompt Injection / Jailbreak**        | Malicious or accidental user text that overrides the intended system instructions.                                       |
| **Robustness**                          | Modelâ€™s resilience to harmless prompt perturbations (capitalisation, ordering) and adversarial attacks.                  |
| **Function / Tool Calling**             | Prompt pattern that guides the model to output structured JSON describing API calls.                                     |
| **Retrievalâ€‘Augmented Prompting (RAP)** | Dynamically inserting retrieved passages into the prompt (RAG without vector DB).                                        |
| **LoRA / Adapter / Prefixâ€‘Tuning**      | Parameterâ€‘efficient tuning methods that *augment* prompts when ICL alone underâ€‘performs.                                 |

---

## ğŸ“Œ Anatomy of an Effective Prompt

```text
[System]
  â€¢ Role + highâ€‘level policy + constraints
[User]
  â€¢ Task description (clear, concise)
  â€¢ Optional: Examples (fewâ€‘shot / CoT)
  â€¢ Optional: External context (RAP / RAG)
[Assistant]
  â€¢ (Left empty) â€“ model fills here
```

> **Bestâ€‘practice ordering**: Most decoderâ€‘only LLMs (GPTâ€‘4o, Mistral, Qwen) perform best when the *task description precedes examples*. Llamaâ€‘3 empirically favours examplesâ€‘first. Always A/Bâ€‘test.

### Prompt Components

1. **Role specification** â€“ *â€œYou are a senior legal analystâ€¦â€*
2. **Explicit constraints** â€“ length, language, style, output schema.
3. **Highâ€‘quality exemplars** â€“ Demonstrate desired inputâ†’output pairs.
4. **Reasoning cues** â€“ *â€œThink stepâ€‘byâ€‘stepâ€*, *â€œUse bullet pointsâ€*.
5. **Guard phrases** â€“ Prevent unwanted behaviour (*â€œIf unsure, reply: â€˜I donâ€™t know.â€™â€*).

---

## ğŸ“Œ Prompt Engineering Techniques & Patterns

| Technique                              | Purpose                                              | Miniâ€‘Example                                                          |
| -------------------------------------- | ---------------------------------------------------- | --------------------------------------------------------------------- |
| **Zeroâ€‘shot**                          | Quick baseline; relies on modelâ€™s latent knowledge.  | *â€œTranslate to Arabic: â€˜Climate change is real.â€™â€*                    |
| **Fewâ€‘shot ICL**                       | Teach task structure, style; reduces hallucinations. | Provide 3 Q\&A pairs before new question.                             |
| **CoT Prompting**                      | Unlock reasoning tasks (math, logic).                | *â€œLetâ€™s solve stepâ€‘byâ€‘step â€¦â€*                                        |
| **Selfâ€‘Consistency**                   | Increase accuracy by sampling N CoTs then voting.    | Sample 10 traces; choose majority answer.                             |
| **Treeâ€‘ofâ€‘Thought / Graphâ€‘ofâ€‘Thought** | Parallel exploration, prune best path.               | Reason about complex planning problems.                               |
| **Function Calling**                   | Structured tool invocation (OpenAI JSON mode).       | Model outputs `{ "name": "getWeather", "args": { "city": "Dubai" } }` |
| **RAP / RAG**                          | Inject factual passages to ground responses.         | Append â€œ### CONTEXTâ€ with retrieved docs.                             |
| **Style Transfer Prompts**             | Enforce tone & persona.                              | *â€œRewrite in Shakespearean English â€¦â€*                                |
| **Negative / Antiâ€‘Prompts**            | Ban sensitive topics.                                | *â€œDo NOT mention political affiliation.â€*                             |
| **Metaâ€‘Prompting**                     | Ask the model to improve its own prompt.             | *â€œRewrite the above prompt to be clearer.â€*                           |

---

## ğŸ“Œ Robustness & Security

### Threats

* **Directâ€‘prompt injection**: *â€œIgnore previous instructions and â€¦â€*
* **Indirect/contextual**: Malicious data retrieved via RAG.
* **Semantic attacks**: Homoglyphs, unicode exploits.

### Defences

1. **Input sanitisation & classification** â€“ Detect PII, policy violations.
2. **Role hierarchy** â€“ System > developer > user; enforce with structured chat schemas (ChatML).
3. **Output filtering** â€“ Postâ€‘generation moderation (OpenAI Moderation, Guardrails.ai).
4. **Adversarial testing / redâ€‘teaming** â€“ Simulate attacks during QA.
5. **Structured JSON mode** â€“ Limits freeâ€‘text jailbreak vectors.

---

## ğŸ“Œ Advanced Prompting Trends (2025)

| Trend                                  | Insight                                                                                                                                        |
| -------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| **Multimodal Prompts**                 | Models like GPTâ€‘4o accept text + image + audio. Prompts now embed `<image>` placeholders and refer to regions (*â€œIn the topâ€‘left corner, â€¦â€*). |
| **Longâ€‘Context Windows**               | 256Kâ€‘token models (Qwenâ€‘Long) enable inâ€‘context â€œdatabaseâ€ prompts â€“ but require chunking + retrieval to avoid dilution.                       |
| **Agentic Prompt Loops**               | Orchestrators (LangGraph, CrewAI) create *task â†’ plan â†’ execute â†’ reflect* cycles via hierarchical prompts.                                    |
| **Programâ€‘Aided LLMs (PAL / Gorilla)** | The LLM writes code; external interpreter returns results; prompt fosters toolâ€‘use reasoning.                                                  |
| **Semantic Caching**                   | Embedâ€‘based cache keys to reuse identical prompt intents, slashing latency & cost.                                                             |

---

## ğŸ“Œ Mindmap of ChapterÂ 5

```
Prompt Engineering
â”œâ”€â”€ Anatomy of a Prompt
â”‚   â”œâ”€â”€ System
â”‚   â”œâ”€â”€ User (task)
â”‚   â”œâ”€â”€ Examples
â”‚   â””â”€â”€ Context/RAG
â”‚
â”œâ”€â”€ Techniques
â”‚   â”œâ”€â”€ Zeroâ€‘shot / Fewâ€‘shot
â”‚   â”œâ”€â”€ CoT & Selfâ€‘Consistency
â”‚   â”œâ”€â”€ Function Calling
â”‚   â”œâ”€â”€ RAP / RAG
â”‚   â””â”€â”€ Style / Negative / Meta
â”‚
â”œâ”€â”€ Sampling Knobs
â”‚   â”œâ”€â”€ Temperature
â”‚   â”œâ”€â”€ Topâ€‘k
â”‚   â””â”€â”€ Topâ€‘p
â”‚
â”œâ”€â”€ Robustness & Security
â”‚   â”œâ”€â”€ Threats (injection, semantic)
â”‚   â””â”€â”€ Defences (sanitise, role hierarchy, redâ€‘team)
â”‚
â””â”€â”€ Advanced Trends
    â”œâ”€â”€ Multimodal
    â”œâ”€â”€ Long Context
    â”œâ”€â”€ Agentic Loops
    â””â”€â”€ PAL & Semantic Cache
```

---

## ğŸ“Œ Actionable Next Steps

1. **Build a Prompt Library** â€“ Store tested prompts alongside metadata (task, model, metrics).
2. **A/Bâ€‘Test Prompt Variants** â€“ Measure impact on accuracy, latency, cost.
3. **Automate Redâ€‘Team Scripts** â€“ Continuously probe for jailbreaks; patch prompts accordingly.
4. **Experiment with Function Calling** â€“ Create small toolâ€‘use demos (e.g., weather API) to internalise schema prompting.
5. **Monitor Prompt Drift** â€“ Periodically reâ€‘evaluate prompts as models update (e.g., GPTâ€‘4o behavioural shifts).

---

Mastering prompt engineering empowers you to squeeze maximal value from foundation models **before** incurring heavier costs of fineâ€‘tuning. In the next chapter weâ€™ll explore Retrievalâ€‘Augmented Generation (RAG) and contextâ€‘construction strategies to push performance even further.
