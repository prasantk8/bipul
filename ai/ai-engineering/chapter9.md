# ğŸš€ Chapter 9 â€” **Inference Optimisation & Cost Engineering**

> **Why it matters**Â â€” a brilliant model that costs \$0.50/query or responds in 10â€¯s will never hit production scale. This chapter delivers concrete techniques to **speedâ€‘up**, **scaleâ€‘out**, and **costâ€‘down** inference, blending modern research (2024â€‘2025) with handsâ€‘on lessons from companies like OpenAI, Mistral, and Netflix.

---

## 1ï¸âƒ£  Key Concepts & Metrics

| Term                  | Definition                                        | Typical Target                           |
| --------------------- | ------------------------------------------------- | ---------------------------------------- |
| **Latency (p95)**     | 95thâ€‘percentile time from requestÂ â†’ first token.  | <Â 500â€¯ms for chat UI.                    |
| **Throughput**        | Tokens/sec/GPU or queries/sec/pod.                | â‰¥Â 150Â tok/s for 70â€¯BÂ @Â A100.             |
| **Tokens per Dollar** | Generated tokens divided by infra \$ cost.        | >Â 1â€¯M tok/\$ for 7â€¯B QLoRA.              |
| **Computeâ€‘Bound**     | Limited by FLOPS capability.                      | Use larger GPU clusters.                 |
| **Memoryâ€‘BWâ€‘Bound**   | Limited by RAM â†” GPU copy bandwidth.              | Use fusedâ€‘kernel libs, KVâ€‘cache pinning. |
| **Batch Size**        | Concurrent sequences per forward pass.            | Autoâ€‘tune (vâ€‘shaped vs latency).         |
| **KV Cache**          | Attention keyâ€‘value tensors reused across tokens. | Pin in HBM to cut decode cost 3â€‘5Ã—.      |

---

## 2ï¸âƒ£  Bottleneck Identification Cheatsheet

| Symptom                                            | Likely Cause                                | Fix                                         |
| -------------------------------------------------- | ------------------------------------------- | ------------------------------------------- |
| Steady 300â€¯ms *per* new token, regardless of batch | **Decodeâ€‘bound** (attention scaling O(nÂ²)). | Flashâ€‘Attentionâ€‘2; specâ€‘distil to 4â€‘8K ctx. |
| Big jump on first token, fast thereafter           | Embedding lookup, network overhead.         | Preâ€‘load vocab on GPU; HTTP/2 keepâ€‘alive.   |
| GPU util <â€¯40â€¯%                                    | Small batch, I/O wait.                      | Enable dynamic batching (vLLM, TGI).        |
| p95 latency spikes nightly                         | Coldâ€‘start pods or autoscaler churn.        | Preâ€‘warm minâ€‘replicas; scheduled scaleâ€‘ups. |

---

## 3ï¸âƒ£  Modelâ€‘Level Optimisations

### 3.1  Quantisation Matrix

| Bits            | Speed â†‘ | Memory â†“ | Quality â–³ | Prod Example                      |
| --------------- | ------- | -------- | --------- | --------------------------------- |
| **16â€‘bit FP16** | 1.0Ã—    | 1.0Ã—     | 0         | OpenAI GPTâ€‘3.5 prod.              |
| **8â€‘bit Int**   | 1.3Ã—    | 1.8Ã—     | âˆ’0.5â€¯pp   | Mistral Instructâ€‘8bit API.        |
| **4â€‘bit NF4**   | 1.8Ã—    | 4Ã—       | âˆ’1â€¯pp     | HuggingÂ Face QLoRA models.        |
| **GPTQ 3â€‘bit**  | 2.4Ã—    | 5â€‘6Ã—     | âˆ’2â€‘3â€¯pp   | Geekbench Llamaâ€‘3â€‘inâ€‘yourâ€‘pocket. |

> **Ruleâ€‘ofâ€‘thumb**Â â€” accept â‰¤Â 1â€¯pp quality drop for 2â€‘4Ã— cost cut. Always evaluate on *your* task.

### 3.2  Operator Fusion & Flash Attention

* **FlashAttentionâ€‘2** (DaoÂ 2024)Â â†’ 1.5â€‘1.9Ã— speed on A100/H100.
* **SMoEbert** gating fused kernelsÂ â†’ 3â€¯% util gain.
* Use **TRTâ€‘LLM** or **vLLM Triton** autopatched kernels.

### 3.3  Lowâ€‘Rank Adapter Merging at Serveâ€‘Time

* Merge LoRA into base, then quantise â†’ avoid extra matmuls.
* Netflix MediaGen saw 18â€¯% latency reduction.

---

## 4ï¸âƒ£  Serviceâ€‘Level Optimisations

### 4.1  Modern Inference Runtimes

| Runtime                      | Killer Feature                                          | When to Use                  |
| ---------------------------- | ------------------------------------------------------- | ---------------------------- |
| **vLLM**                     | Pagedâ€‘attention KV cache, dynamic batching autoâ€‘engine. | Chat apps, high fanâ€‘out.     |
| **TGI (TextÂ Gen Inference)** | Multiâ€‘GPU tensorâ€‘parallel, secure JWT.                  | Enterprise onâ€‘prem clusters. |
| **TRTâ€‘LLM**                  | TensorRT graph optim + quant pipelines.                 | Nvidia GPU datacentres.      |
| **ONNXâ€‘Runtimeâ€‘LLM**         | Crossâ€‘vendor CPU/GPU; Windows edge.                     | Local desktop, Xbox.         |

### 4.2  Dynamic Batching Logic (Pseudocode)

```python
while queue.not_empty():
    batch = gather(max_tokens=2048, timeout=5ms)
    run_inference(batch)
```

* **Tradeâ€‘off**Â â€” bigger batch â‡’ throughput â†‘, latency â†‘ until you cross p95 SLO.

### 4.3  KVâ€‘CacheÂ Sharding

* For 32â€‘sequence batch of Llamaâ€‘70B, KV = 23â€¯GB context.
* vLLM pages inactive cache to host pinned memory, retaining 90â€¯% speed.

---

## 5ï¸âƒ£  Hardware & Cluster Tuning

| Lever            | Observation                               | Guideline                                     |
| ---------------- | ----------------------------------------- | --------------------------------------------- |
| **GPU Type**     | A100 vs H100Â â†’ H100 \~1.7Ã— decode TPS.    | If SLO <500â€¯ms and traffic >20Â QPS, use H100. |
| **CPUâ€‘GPU PCIe** | PCIeÂ 3.0 halves KV copy speed vÂ 4.0.      | Ensure PCIeÂ Gen4+ or NVLink.                  |
| **Networking**   | Token streaming relies on TCP push.       | Enable TCP\_NODELAY; gRPC streaming.          |
| **Autoscaling**  | Sudden viral load at OpenAI (MarchÂ 2025). | Predictive Kâ€‘means on last 7â€‘days traffic.    |

---

## 6ï¸âƒ£  Realâ€‘World Case Studies

### 6.1  Airbnb AI Concierge (2024)

* Problem: 40â€¯K RPS spikes every Friday.
* Solution: vLLM + 8â€‘bit quant + multiâ€‘region cache.
* Result: p90 latency 820â†’290â€¯ms; infra bill âˆ’47â€¯%/month.

### 6.2  BloombergGPTâ€‘FinServe (2025 private cloud)

* Mixtralâ€‘8x22B MoE with TRTâ€‘LLM int4.
* Deployed on 32 H100 DGX nodes.
* Achieved 4.2â€¯M tok/\$ vs 1.6â€¯M baseline.

### 6.3  Meta ThreadsÂ AI

* Onâ€‘device Llamaâ€‘3â€‘8B.GGUF 3â€‘bit (iOS).
* Distilled vocabulary to 35â€¯K; runtime â‰¤Â 80â€¯ms.

---

## 7ï¸âƒ£  Costâ€‘Control Playbook

1. **Quantise first, distil second.**
2. **Enable semantic cache** (>25â€¯% hit saves decode cost).
3. **Use smaller expert model for firstâ€‘tokens** (Speculative decoding ala ChenÂ 2024).
4. **Leverage Spot/Spare Instances** for traffic <Â SLO; Netflix saved 30â€¯%.
5. **Monitor *tokens/user* metric** â€” drives cost more than QPS.

---

## 8ï¸âƒ£  Monitoring & Alerting

| Metric      | Alert if                  | Tool                  |
| ----------- | ------------------------- | --------------------- |
| p95 Latency | >Â SLO 800â€¯ms 5â€¯min window | Prometheus + Grafana. |
| GPU Util    | <Â 50â€¯% 10â€¯min             | Nvidia DCGM exporter. |
| Errorâ€‘Rate  | >Â 0.1â€¯%                   | Sentry.               |
| Cache Hit % | â†“20â€¯% vs baseline         | Custom exporter.      |

Dashboard sample (PromQL):

```promql
histogram_quantile(0.95, rate(llm_request_duration_seconds_bucket[5m]))
```

---

## 9ï¸âƒ£  Frontier Research (2025)

* **Speculative Decoding v2**Â â€” 5Ã— throughput using 2â€‘stage studentâ€“teacher (DeepMind Geminiâ€‘Spec).
* **Fused Multiâ€‘Query MoE Attention**Â â€” reduces KV size by factor of experts (Ref: MicrosoftÂ NARU 2025).
* **Confidential Inference (TEEs)**Â â€” AzureÂ Cobalt; enclaves add <7â€¯% overhead for regulated data.
* **Liquidâ€‘Cooling Microâ€‘pods**Â â€” 35â€¯% energy savings (Meta EU Datacentres).

---

## ğŸ”š  Key Takeaways

1. **Measure first** â€” profile to know if you are computeâ€‘bound, memâ€‘bwâ€‘bound, or I/Oâ€‘bound.
2. **Quant + FlashAttn + Dynamic Batch** â†’ 2â€‘4Ã— cost/perf wins with minimal quality drop.
3. **Choose runtime wisely** â€” vLLM for chat, TRTâ€‘LLM for GPUâ€‘dense clusters, ONNX for edge.
4. **Monitor & autoâ€‘scale** â€” demand is spiky; guard p95 latency SLO.
5. **Iterate** â€” new kernels (Flashâ€‘AttnÂ 3) or quant methods arrive monthly.

> â€œInference optimisation is **continuous DevOps**, not a oneâ€‘off tuning sprint.â€
